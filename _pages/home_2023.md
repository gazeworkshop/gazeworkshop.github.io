---
layout: layout_2023
urltitle:  "GAZE 2023: Gaze Estimation and Prediction in the Wild"
title: "GAZE 2023: Gaze Estimation and Prediction in the Wild"
categories: cvpr, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /2023/
favicon: /2023/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
# Nika_Akin
---

<br>
<div class="row">
  <div class="col-xs-12">
    <p><center>
    <img class="img-fluid" width="900" height="400" src="{{ "img/banner.png" | prepend:site.baseurl }}">
    <!-- <small style="float:right;margin-top:1mm;margin-right:12mm;">Image credit to <a href="https://pixabay.com/illustrations/eye-watercolor-art-sketch-4453129/" target="_blank">Nika_Akin</a></small> -->
    </center></p>
    <p><center>
      Sunday Morning, <font size="3" color="red">18th June 2023</font> (half-day)
      <br><br>
      <b>5 papers</b> have been accepted in our GAZE2023 workshop.
      <br>
      Congratulations to all authors!
      <!-- Coming soon -->
      <!-- 3pm - 8:20pm UTC -->
      <!-- <table id="top-table">
        <style>
	  #top-table td {
	    padding: 1px 5px;
	  }
	  #top-table td:nth-child(1),
	  #top-table td:nth-child(3) {
	    text-align: right;
	    padding-right: 0px;
	  }
	</style>
        <tr><td>8am   </td><td>- 1:20pm     </td><td> PDT</td><td>(UTC-7)  </td></tr>
        <tr><td>11am  </td><td>- 4:20pm     </td><td> EDT</td><td>(UTC-4)  </td></tr>
        <tr><td>4pm   </td><td>- 9:20pm     </td><td> BST</td><td>(UTC+1)  </td></tr>
        <tr><td>5pm   </td><td>- 10:20pm    </td><td>CEST</td><td>(UTC+2)  </td></tr>
        <tr><td>8:30pm</td><td>- 1:50am (+1)</td><td> IST</td><td>(UTC+5.5)</td></tr>
        <tr><td>11pm  </td><td>- 4:20am (+1)</td><td> CST</td><td>(UTC+8)  </td></tr>
        <tr><td>12am  </td><td>- 5:20am (+1)</td><td> KST</td><td>(UTC+9)  </td></tr>
      </table> -->
      <!-- <br>
      Youtube recording: <a href="https://youtu.be/WQ8azMW_dn8" target="_blank">https://youtu.be/WQ8azMW_dn8</a> -->
    </center></p>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 5th International Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2023) at <a href="http://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> aims to encourage and highlight novel strategies for eye gaze estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, both spatially and temporally.
This is expected to be achieved by applying novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training.
Specifically, the workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Reformulating eye detection, gaze estimation, and gaze prediction pipelines with deep networks.</li>
      <li>Applying geometric and anatomical constraints into the training of (sparse or dense) deep networks.</li>
      <li>Leveraging additional cues such as contexts from face region and head pose information.</li>
      <li>Developing adversarial methods to deal with conditions where current methods fail (illumination, appearance, etc.).</li>
      <li>Exploring attention mechanisms to predict the point of regard.</li>
      <li>Designing new accurate measures to account for rapid eye gaze movement.</li>
      <li>Novel methods for temporal gaze estimation and prediction including Bayesian methods.</li>
      <li>Integrating differentiable components into 3D gaze estimation frameworks.</li>
      <li>Robust estimation from different data modalities such as RGB, depth, head pose, and eye region landmarks.</li>
      <li>Generic gaze estimation method for handling extreme head poses and gaze directions.</li>
      <li>Temporal information usage for eye tracking to provide consistent gaze estimation on the screen.</li>
      <li>Personalization of gaze estimators with few-shot learning.</li>
      <li>Semi-/weak-/un-/self- supervised leraning methods, domain adaptation methods, and other novel methods towards improved representation learning from eye/face region images or gaze target region images.</li>
    </ul>
We will be hosting 2 invited speakers for the topic of gaze estimation. We will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. More information will be provided as soon as possible.
  </div>
</div> <br>

<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://cvpr2022.thecvf.com/author-guidelines" target="_blank">CVPR format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at this <a href="https://cmt3.research.microsoft.com/GAZE2023/Submission/Index">CMT link</a>.
	    <!-- <a href="https://cmt3.research.microsoft.com/GAZE2021/Submission/Index" target="_blank" title="CMT Submission System for GAZE 2021">this CMT link</a>. -->
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
	    <!--Due to potential clashes with the main conference reviewing schedule, we will accept simultaneous submissions to the ICCV main conference and GAZE Workshop. Simultaneous submissions are otherwise disallowed.-->
          </p>
        </div>
      </div>
    </div>
    <br>
    <!--
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-ea" style="cursor:pointer;">
        <h3 style="margin:0;">Extended Abstracts</h3>
      </div>
      <div id="call-ea" class="panel-collapse collapse in" data-parent="#call">
        <div class="panel-body">
          <p>
            In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ECCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.
          </p>
	  <p>
	    Further details on how this poster session will occur online, is to be decided. In general, we will be following the main ECCV conference guidelines and organization in organizing the presentation of all posters.
          </p>
          <p>Extended abstracts are limited to six pages and must be created using the <a href="https://eccv2020.eu/author-instructions/" target="_blank">official ECCV format</a>. The submission must be sent to <a href="mailto:openeyes.workshop@gmail.com">openeyes.workshop@gmail.com</a> by the deadline (July 17th).
          </p>
          <p>
            We will evaluate and notify authors of acceptance as soon as possible (evaluation on a rolling basis until the deadline) after receiving their extended abstract submission.
          </p>
        </div>
      </div>
    </div>
    <br>
    -->
    <!-- <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-challenge" style="cursor:pointer;">
        <h3 style="margin:0;">GAZE 2021 Challenges</h3>
      </div>
      <div id="call-challenge" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
    Coming soon
	  <p>
	    The GAZE 2021 Challenges are hosted on Codalab, and can be found at:
	  </p>
	  <ul>
	    <li>ETH-XGaze Challenge: <a href="https://competitions.codalab.org/competitions/28930">https://competitions.codalab.org/competitions/28930</a></li>
	    <li>EVE Challenge: <a href="https://competitions.codalab.org/competitions/28954">https://competitions.codalab.org/competitions/28954</a></li>
	  </ul>
	  <p>
            More information on the respective challenges can be found on their pages.
	  </p>
	  <br>
	  <p>
	    We are thankful to our sponsors for providing the following prizes:
	    <table style="width: 100%;">
	      <colgroup>
                <col span="1" style="width: 30%;">
		<col span="1" style="width: 55%;">
		<col span="1" style="width: 15%;">
	      </colgroup>
	      <tbody>
	        <tr>
	          <td><b>ETH-XGaze Challenge Winner</b></td>
	          <td>USD 500</td>
	          <td><small>courtesy of </small><img width="50" src="img/google.png"/></td>
	        </tr>
	        <tr>
	          <td><b>EVE Challenge Winner</b></td>
	          <td>Tobii Eye Tracker 5</td>
	          <td><small>courtesy of </small><img width="50" src="img/tobii.jpg"/></td>
	        </tr>
	      </tbody>
	    </table>
	  </p>
        </div>
      </div>
    </div> -->
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Released</td>
          <td>February 13, 2021</td>
        </tr> -->
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 13, 2023 (12:00 Pacific time)</td>
	  <td><span class="countdown" reference="13 Mar 2023 12:00:00 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>Mar 31, 2023</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 8, 2023</td>
        </tr>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Closed</td>
          <td>May 28, 2021 (23:59 UTC)</td>
	  <td><span class="countdown" reference="28 May 2022 23:59:59 UTC"></span></td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule</h2>
     <br>
     <!-- <p>
       Attending:
       <ul>
         <li>Registered CVPR attendees can find the relevant Zoom and Gatherly links at <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/login.asp">https://www.eventscribe.net/2021/2021CVPR/login.asp</a></li>
	 <li>Others are welcome to join our livestream at <a href="https://youtu.be/ScoHuri_3hs">https://youtu.be/ScoHuri_3hs</a></li>
       </ul>
     </p> -->
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>3:25pm - 3:30pm</td>
          <td class="to-local-time">18 Jun 2023 15:25:00 UTC</td>
          <td>Opening remark</td>
        </tr>
        <tr>
          <td>3:30pm - 4:10pm</td>
          <td class="to-local-time">18 Jun 2023 15:30:00 UTC</td>
          <td>Invited talk by Erroll Wood</td>
        </tr>
        <tr>
          <td>4:10pm - 4:50pm</td>
          <td class="to-local-time">18 Jun 2023 16:10:00 UTC</td>
          <td>Invited talk by Ruth Rosenholtz</td>
        </tr>
        <tr>
          <td>4:50pm - 5:10pm</td>
          <td class="to-local-time">18 Jun 2023 16:50:00 UTC</td>
          <td>Invited poster spotlight talk</td>
        </tr>
        <tr>
          <td>5:10pm - 6:00pm</td>
          <td class="to-local-time">18 Jun 2023 17:10:00 UTC</td>
          <td>Coffee break & poster presentation</td>
        </tr>
        <tr>
          <td>6:00pm - 6:50pm</td>
          <td class="to-local-time">18 Jun 2023 18:00:00 UTC</td>
          <td>Workshop paper presentation</td>
        </tr>
        <!-- <tr>
          <td>6:50pm - 7:00pm</td>
          <td class="to-local-time">18 Jun 2022 18:50:00 UTC</td>
          <td>Panel discussion</td>
        </tr> -->
        <tr>
          <td>6:50pm - 7:00pm</td>
          <td class="to-local-time">18 Jun 2023 18:50:00 UTC</td>
          <td>Award & closing remark</td>
        </tr>
        <!-- <tr>
          <td>8:15pm - 8:20pm</td>
          <td class="to-local-time">20 Jun 2021 20:15:00 UTC</td>
          <td>Award & closing remark</td>
        </tr> -->
      </tbody>
     </table>
     <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="http://persci.mit.edu/people/rosenholtz">
          <img class="people-pic" src="/2023/img/people/rr.jpg" />
        </a>
        <div class="people-name">
          <a href="http://persci.mit.edu/people/rosenholtz">Ruth Rosenholtz</a>
          <h6>Massachusetts Institute of Technology</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Human vision at a glance</h3><br />
        <b>Abstract</b>
        <p class="speaker-abstract">Research at the GAZE 2023 workshop aims to estimate and predict where someone is looking. But if we know someone's direction of gaze, what can we say about what they see, and what parts of the visual input they process? One cannot simply assume that observers process the object lying at the point of regard. Human visual perception gathers considerable information over a fairly wide field of view. People may not always point their eyes at the interesting bits, because that may not always be optimal for real-world tasks.</p>

        <p class="speaker-abstract">One can make sense of these questions through understanding of the strengths and limitations of human peripheral vision. We move our eyes as part of a complex tradeoff between the information available in the fovea vs. periphery, and the costs of shifting one's gaze. Furthermore, this tradeoff depends on factors such as individual differences, age, and level of experience. Recent understanding and modeling of peripheral vision can provide important insights into what a person can see, given where they look. This understanding could also help determine such things as the cost of misestimating someone's direction of gaze.</p>

        <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
            Ruth Rosenholtz is a Principal Research Scientist in MIT’s Department of Brain and Cognitive Sciences, a member of CSAIL, and currently on sabbatical at NVIDIA Research. She has a Ph.D. in EECS (Computer Vision) from UC Berkeley. She joined MIT in 2003 after 7 years at the Palo Alto Research Center (formerly Xerox PARC). Her work focuses on developing predictive models of human visual processing, including peripheral vision, visual search, visual clutter, and perceptual organization. In addition, her lab works on applying understanding of human vision to image fidelity (NASA Ames), and to design of user interfaces and information visualizations (Xerox PARC and MIT). She is a world expert in peripheral vision and its implications for how we think about vision, attention, and design.
            </p>
          </div></div>
        </div>
      </div>
    </div>
    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://errollw.com">
          <img class="people-pic" src="/2023/img/people/ew.jpeg" />
        </a>
        <div class="people-name">
          <a href="https://errollw.com">Erroll Wood</a>
          <h6>Google</h6>
        </div>
      </div>
      <div class="col-sm-9">
	<h3>Synthetics for Gaze Estimation and More</h3><br />
	<b>Abstract</b><p class="speaker-abstract">
    Nowadays, collecting the right dataset for machine learning is often more challenging than choosing the model. We address this with photorealistic synthetic training data – labelled images of humans made using computer graphics. With synthetics we can generate clean labels without annotation noise or error, produce labels otherwise impossible to annotate by hand, and easily control variation and diversity in our datasets. I will show you how synthetics underpins our work on understanding humans, including how it enables fast and accurate 3D face tracking, including eye gaze, in the wild.
	</p>
    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/RKP-vhrMcM8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
	<div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#me-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="me-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
            Erroll is a Staff Software Engineer at Google, working on Digital Humans. Previously, he was a member of Microsoft's Mixed Reality AI Lab, where he worked on hand tracking for HoloLens 2, avatars for Microsoft Mesh, synthetic data for face tracking, and Holoportation. He did his PhD at the University of Cambridge, working on gaze estimation.
            </p>
          </div></div>
        </div>
      </div>
    </div>

    <br>

  <!-- <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://people.csail.mit.edu/recasens/">
          <img class="people-pic" src="/2021/img/people/ar.jpg" />
        </a>
        <div class="people-name">
          <a href="https://people.csail.mit.edu/recasens/">Adrià Recasens</a>
          <h6>DeepMind</h6>
        </div>
      </div>
      <div class="col-sm-9">
	<h3>Where are they looking?</h3><br />
	<b>Abstract</b><p class="speaker-abstract">
	In order to understand actions or anticipate intentions, humans need efficient ways of gathering information about each other. In particular, gaze is a rich source of information about other peoples’ activities and intentions. In this talk, we describe our work on predicting human gaze. We introduce a series of methods to follow gaze for different modalities. First, we present GazeFollow, a dataset and model to predict the location of people's gaze in an image. Furthermore, we introduce Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze direction estimation in unconstrained scenes. Finally, we also propose a saliency-based sampling layer designed to improve performance in arbitrary tasks by efficiently zooming into the relevant parts of the input image.
	</p>
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#ar-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="ar-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
	    Adrià Recasens is a Research Scientist at DeepMind. He previously completed his PhD on computer vision at the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in 2019. During his PhD, he worked on various topics related to image and video understanding. Particularly, he has various publications on gaze estimation on image and video. His current research focuses on self-supervised learning specifically applied to multiple modalities such as video, audio or text.
            </p>
          </div></div>
        </div>
      </div>
    </div>
  </div>
</div> -->
  <br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="awards"></a>
    <h2>Awards</h2>
    TBD

  <!-- <div class="award">
      <h3>
	    <b> Best Paper Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.nvidia.com/" target="_blank"><img src="img/nvidia.jpg" /></a>
	    </span>
      </h3>
      <p><br>
	Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation<br>
	<i>Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>

  <div class="award">
      <h3>
	    <b> Best Poster Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.google.com/" target="_blank"><img src="img/google.png" /></a>
	    </span>
      </h3>
      <p><br>
	Unsupervised Multi-View Gaze Representation Learning<br>
	<i>John Gideon, Shan Su, Simon Stent</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>
  </div>
</div><br> -->

   <br><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers</h2>

  <div class="paper">
        <span class="title">Multimodal Integration of Human-Like Attention in Visual Question Answering</span>
        <span class="authors">Ekta Sood (University of Stuttgart); Fabian Kögel (Sony Europe B.V.); Philipp Müller (DFKI GmbH); Dominike Thomas (University of Stuttgart); Mihai Bace (University of Stuttgart); Andreas Bulling (University of Stuttgart)</span>
        <!-- <span class="award">Best Paper Award</span> -->
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2023</button>
	  <!-- <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a> -->
        </div>
    </div>

  <div class="paper">
        <span class="title">Kappa Angle Regression with Ocular Counter-Rolling Awareness for Gaze Estimation</span>
        <span class="authors">Shiwei Jin (UCSD); Ji Dai (UCSD); Truong Nguyen (UC San Diego)</span>
        <!-- <span class="award">Best Paper Award</span> -->
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2023</button>
	  <!-- <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a> -->
        </div>
    </div>

  <div class="paper">
        <span class="title">GazeCaps: Gaze Estimation with Self-Attention-Routed Capsules</span>
        <span class="authors">Hengfei Wang (University of Birmingham); Jun O Oh (Dankook University); Hyung Jin Chang (University of Birmingham); Jin Hee Na (VTouch Inc.); Minwoo Tae (Dankook University); Zhongqun Zhang (University of Birmingham); Sang-Il Choi (Dankook University)</span>
        <!-- <span class="award">Best Paper Award</span> -->
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2023</button>
	  <!-- <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a> -->
        </div>
    </div>

  <div class="paper">
        <span class="title">Where are they looking in the 3D space?</span>
        <span class="authors">Nora Horanyi (University of Birmingham ); Linfang Zheng (University of Birmingham); Eunji Chong (Amazon); Ales Leonardis (University of Birmingham ); Hyung Jin Chang (University of Birmingham)</span>
        <!-- <span class="award">Best Paper Award</span> -->
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2023</button>
	  <!-- <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a> -->
        </div>
    </div>

  <div class="paper">
        <span class="title">EFE: End-to-end Frame-to-Gaze Estimation</span>
        <span class="authors">Haldun Balim (ETH Zurich); Seonwook Park (Lunit Inc.); Xi Wang (ETH Zurich); Xucong Zhang (Delft University of Technology); Otmar Hilliges (ETH Zurich)</span>
        <!-- <span class="award">Best Paper Award</span> -->
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2023</button>
	  <!-- <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a> -->
        </div>
    </div>

  <!-- <a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters</h2>

  <div class="paper">
        <span class="title">Dynamic 3D Gaze from Afar: Deep Gaze Estimation from Temporal Eye-Head-Body Coordination</span>
        <span class="authors">Soma Nonaka, Shohei Nobuhara, Ko Nishino</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22.pdf"><i class="fas fa-file-pdf"></i> PDF</a>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22_supp.pdf"><i class="fas fa-file-pdf"></i> Supp.</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/kyotovision-public/dynamic-3d-gaze-from-afar"><i class="fas fa-code"></i> Code</a>
	  <a class="btn btn-default" target="_blank" href="https://youtu.be/IEc8E4e4mXU"><i class="fas fa-video"></i> Video</a>
        </div>
    </div> -->

  </div>
</div>
<br><br>

<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://ejcgt.github.io/">Eunji Chong</a><h6>Amazon</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyf015.github.io/">Yifei Huang</a><h6>University of Tokyo</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.lunit.io/en">Heon Song</a><h6>Lunit Inc.</h6></div>
    <div class="people-name"><a target="_blank" href="https://scholar.google.com/citations?user=nQxMQxQAAAAJ&hl=en">Rafael Cabeza</a><h6>Public University of Navarre</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://www.biostat.wisc.edu/~yli/">Yin Li</a><h6>University of Wisconsin-Madison</h6></div>
    <div class="people-name"><a target="_blank" href="https://ejcgt.github.io/">Marcel Bühler</a><h6>ETH Zürich</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.ecse.rpi.edu/~qji/">Qiang Ji</a><h6>Rensselaer Polytechnic Institute</h6></div>
    <div class="people-name"><a target="_blank" href="https://kuangcy1998.wixsite.com/luckyspace">Chenyi Kuang</a><h6>Rensselaer Polytechnic Institute</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="">Jiawei Qin</a><h6>The University of Tokyo</h6></div>
    <div class="people-name"><a target="_blank" href="https://hengfei-wang.github.io//github.io/">Hengfei Wang</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://research.monash.edu/en/persons/shreya-ghosh">Shreya Ghosh</a><h6>Monash University</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.hci.uni-tuebingen.de/chair/team/wolfgang-fuhl">Wolfgang Fuhl</a><h6>University of Tuebingen</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/zhengyuf/">Yufeng Zheng</a><h6>ETH Zurich</h6></div>
  </div>
</div>
<br>

<br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "img/people/hj.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.ccmitss.com/zhang">
      <img class="people-pic" src="{{ "img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>
      <h6>Delft University of Technology</h6>
    </div>
  </div>
  <!-- <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>Lunit Inc.</h6>
    </div>
  </div> -->
  <div class="col-xs-2">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>NVIDIA Research</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://thabobeeler.com/">
      <img class="people-pic" src="{{ "img/people/tb.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://thabobeeler.com/">Thabo Beeler</a>
      <h6>Google</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://swook.net/">
      <img class="people-pic" src="{{ "img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://swook.net/">Seonwook Park</a>
      <h6>Lunit Inc.</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
  <!-- <div class="col-xs-2">
    <a href="https://www.ecse.rpi.edu/~qji/">
      <img class="people-pic" src="{{ "img/people/qj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ecse.rpi.edu/~qji/">Qiang Ji</a>
      <h6>Rensselaer Polytechnic Institute</h6>
    </div>
  </div> -->
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hengfei-wang.github.io//github.io/">
      <img class="people-pic" src="{{ "img/people/hengfei.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hengfei Wang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <br><br>
  <!-- <div class="col-xs-2"> -->
    Please contact me if you have any question about this website.
    <br>
    Email: hxw080@student.bham.ac.uk
  <!-- </div> -->
</div>
<br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
   <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
</div>
