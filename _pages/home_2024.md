---
layout: layout_2024
urltitle:  "GAZE 2024: Gaze Estimation and Prediction in the Wild"
title: "GAZE 2024: Gaze Estimation and Prediction in the Wild"
categories: cvpr, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /2024/
favicon: /2024/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
# Nika_Akin
---

<br>
<div class="row">
  <div class="col-xs-12">
    <img class="img-fluid" src="{{ "img/banner.png" | prepend:site.baseurl }}">
    <small style="float:right;margin-top:0mm;margin-right:12mm;">Image credit to <a href="https://openai.com/dall-e-3" target="_blank">DALL·E</a></small>
    <br><br>
    <p><center>
      <!-- Monday Morning, 20th June 2022 (half-day) -->
      <br><br>
      <!-- <b>7 papers</b> have been accepted in our GAZE2022 workshop. -->
      <br>
      <!-- Congratulations to all authors! -->
      <br><br>
      <!-- This year, June 19 and 20 marks Juneteenth, a US holiday commemorating the end of slavery in the US, and a holiday of special significance in the US South. We encourage attendees to learn more about Juneteenth and its historical context, and to join the city of New Orleans in celebrating the Juneteenth holiday. You can find out more information about Juneteenth here: <a href="https://cvpr2022.thecvf.com/recognizing-juneteenth" target="_blank">https://cvpr2022.thecvf.com/recognizing-juneteenth</a> -->
      <!-- Coming soon -->
      <br>
      <!-- 3pm - 8:20pm UTC -->
      <br><br>
      <!-- <table id="top-table">
        <style>
	  #top-table td {
	    padding: 1px 5px;
	  }
	  #top-table td:nth-child(1),
	  #top-table td:nth-child(3) {
	    text-align: right;
	    padding-right: 0px;
	  }
	</style>
        <tr><td>8am   </td><td>- 1:20pm     </td><td> PDT</td><td>(UTC-7)  </td></tr>
        <tr><td>11am  </td><td>- 4:20pm     </td><td> EDT</td><td>(UTC-4)  </td></tr>
        <tr><td>4pm   </td><td>- 9:20pm     </td><td> BST</td><td>(UTC+1)  </td></tr>
        <tr><td>5pm   </td><td>- 10:20pm    </td><td>CEST</td><td>(UTC+2)  </td></tr>
        <tr><td>8:30pm</td><td>- 1:50am (+1)</td><td> IST</td><td>(UTC+5.5)</td></tr>
        <tr><td>11pm  </td><td>- 4:20am (+1)</td><td> CST</td><td>(UTC+8)  </td></tr>
        <tr><td>12am  </td><td>- 5:20am (+1)</td><td> KST</td><td>(UTC+9)  </td></tr>
      </table> -->
      <!-- <br>
      Youtube recording: <a href="https://youtu.be/WQ8azMW_dn8" target="_blank">https://youtu.be/WQ8azMW_dn8</a> -->
    </center></p>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 4th International Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2022) at <a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a> aims to encourage and highlight novel strategies for eye gaze estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, both spatially and temporally.
This is expected to be achieved by applying novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training.
Specifically, the workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Reformulating eye detection, gaze estimation, and gaze prediction pipelines with deep networks.</li>
      <li>Applying geometric and anatomical constraints into the training of (sparse or dense) deep networks.</li>
      <li>Leveraging additional cues such as contexts from face region and head pose information.</li>
      <li>Developing adversarial methods to deal with conditions where current methods fail (illumination, appearance, etc.).</li>
      <li>Exploring attention mechanisms to predict the point of regard.</li>
      <li>Designing new accurate measures to account for rapid eye gaze movement.</li>
      <li>Novel methods for temporal gaze estimation and prediction including Bayesian methods.</li>
      <li>Integrating differentiable components into 3D gaze estimation frameworks.</li>
      <li>Robust estimation from different data modalities such as RGB, depth, head pose, and eye region landmarks.</li>
      <li>Generic gaze estimation method for handling extreme head poses and gaze directions.</li>
      <li>Temporal information usage for eye tracking to provide consistent gaze estimation on the screen.</li>
      <li>Personalization of gaze estimators with few-shot learning.</li>
      <li>Semi-/weak-/un-/self- supervised leraning methods, domain adaptation methods, and other novel methods towards improved representation learning from eye/face region images or gaze target region images.</li>
    </ul>
We will be hosting 2 invited speakers for the topic of gaze estimation. We will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. More information will be provided as soon as possible.
  </div>
</div> <br>

<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://cvpr2022.thecvf.com/author-guidelines" target="_blank">CVPR format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at this <a href="https://cmt3.research.microsoft.com/GAZE2022/Submission/Index">CMT link</a>.
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
          </p>
        </div>
      </div>
    </div>
    <br>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 24, 2022 (12:00 Pacific time)</td>
	  <td><span class="countdown" reference="24 Mar 2022 12:00:00 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>April 11, 2022</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 20, 2022</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule</h2>
     <br>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1:30pm - 1:35pm</td>
          <td class="to-local-time">20 Jun 2022 13:30:00 UTC</td>
          <td>Opening remark</td>
        </tr>
        <tr>
          <td>1:35pm - 2:15pm</td>
          <td class="to-local-time">20 Jun 2022 13:35:00 UTC</td>
          <td>Invited talk by Prof. Wei Shen</td>
        </tr>
        <tr>
          <td>2:15pm - 2:55pm</td>
          <td class="to-local-time">20 Jun 2022 14:15:00 UTC</td>
          <td>Invited talk by Prof. Gordon Wetzstein</td>
        </tr>
        <tr>
          <td>2:55pm - 3:00pm</td>
          <td class="to-local-time">20 Jun 2022 14:55:00 UTC</td>
          <td>Invited poster spotlight talk</td>
        </tr>
        <tr>
          <td>3:00pm - 4:00pm</td>
          <td class="to-local-time">20 Jun 2022 15:00:00 UTC</td>
          <td>Coffee break & poster presentation</td>
        </tr>
        <tr>
          <td>4:00pm - 5:10pm</td>
          <td class="to-local-time">20 Jun 2022 16:00:00 UTC</td>
          <td>Workshop paper presentation</td>
        </tr>
        <tr>
          <td>5:10pm - 5:50pm</td>
          <td class="to-local-time">20 Jun 2022 17:10:00 UTC</td>
          <td>Panel discussion</td>
        </tr>
        <tr>
          <td>5:50pm - 6:00pm</td>
          <td class="to-local-time">20 Jun 2022 17:50:00 UTC</td>
          <td>Award & closing remark</td>
        </tr>
      </tbody>
     </table>
     <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://stanford.edu/~gordonwz/">
          <img class="people-pic" src="/2022/img/people/gw.jpg" />
        </a>
        <div class="people-name">
          <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
          <h6>Stanford University</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Eye Tracking Revisited: Applications in Rendering, Displays, Wearable Computing Systems and Emerging Event-based Eye Tracking</h3><br />
        <b>Abstract</b><p class="speaker-abstract">In this talk, we will discuss several emerging applications of eye tracking in AR/VR displays, including ocular parallax rendering and gaze-contingent stereo rendering, as well as wearable computing systems, for example autofocal eyeglasses for presbyopia correction. Moreover, we will discuss emerging technologies for ultra-low-latency eye tracking beyond 10,000 Hz using event sensors.</p>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/FUPpU_sT3LQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
            Gordon Wetzstein is an Associate Professor of <a href="https://ee.stanford.edu/">Electrical Engineering</a> and, by courtesy, of <a href="https://cs.stanford.edu/">Computer Science</a> at <a href="https://www.stanford.edu/">Stanford University</a>. He is the leader of the <a href="https://www.computationalimaging.org/">Stanford Computational Imaging Lab</a> and a faculty co-director of the <a href="https://scien.stanford.edu/">Stanford Center for Image Systems Engineering</a>. At the intersection of computer graphics and vision, computational optics, and applied vision science, Prof. Wetzstein's research has a wide range of applications in next-generation imaging, display, wearable computing, and microscopy systems. Prior to joining Stanford in 2014, Prof. Wetzstein was a Research Scientist at MIT, he received a Ph.D. in Computer Science from the <a href="http://www.cs.ubc.ca/labs/imager/imager.php">University of British Columbia</a> in 2011 and graduated with Honors from the Bauhaus in Weimar, Germany before that. He is the recipient of an NSF CAREER Award, an Alfred P. Sloan Fellowship, an ACM SIGGRAPH Significant New Researcher Award, a Presidential Early Career Award for Scientists and Engineers (PECASE), an SPIE Early Career Achievement Award, a Terman Fellowship, an Okawa Research Grant, the Electronic Imaging Scientist of the Year 2017 Award, an Alain Fournier Ph.D. Dissertation Award, and a Laval Virtual Award as well as Best Paper and Demo Awards at ICCP 2011, 2014, and 2016 and at ICIP 2016.
            </p>
          </div></div>
        </div>
      </div>
    </div>
    <br>

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://shenwei1231.github.io/">
          <img class="people-pic" src="/2022/img/people/ws.jpg" />
        </a>
        <div class="people-name">
          <a href="https://shenwei1231.github.io/">Wei Shen</a>
          <h6>Shanghai Jiao Tong University</h6>
        </div>
      </div>
      <div class="col-sm-9">
	<h3>Detecting Gaze Targets from Images Captured in the Wild</h3><br />
	<b>Abstract</b><p class="speaker-abstract">
 Gaze target detection, i.e., gaze following, plays a crucial role in human-scene interaction understanding tasks. In this talk, I will introduce our recent works on detecting gaze targets from images captured in natural settings, including (1) a dual attention mechanism to combine field-of-view attention and depth attention to locate gaze targets, (2) a 3D sight line guided dual-pathway framework to detect gaze targets in 360-degree images, and (3) a vision transformer to enable end-to-end gaze following. The excellent results on several gaze following datasets show the potential of our methods to increase the scalability of naturalistic gaze measurement.
	</p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/RKP-vhrMcM8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	<div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#me-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="me-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
           Wei Shen is an Associate Professor at the Artificial Intelligence Institute, Shanghai Jiao Tong University, since October 2020. Before that, he was an Assistant Research Professor at the Department of Computer Science, Johns Hopkins University. His research interests lie in the fields of computer vision, machine learning, and deep learning, particularly in object detection and segmentation, representation learning and human-centered computer vision. He is the recipient of three Natural Science Foundation of China grants. He is an area chair for CVPR 2022 and ACCV 2022, a senior program committee member for AAAI 2022 and an associate editor for Neurocomputing.
            </p>
          </div></div>
        </div>
      </div>
    </div>

  <br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="awards"></a>
    <h2>Awards</h2>

  <div class="award">
      <h3>
	    <b> Best Paper Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.nvidia.com/" target="_blank"><img src="img/nvidia.jpg" /></a>
	    </span>
      </h3>
      <p><br>
	Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation<br>
	<i>Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>

  <div class="award">
      <h3>
	    <b> Best Poster Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.google.com/" target="_blank"><img src="img/google.png" /></a>
	    </span>
      </h3>
      <p><br>
	Unsupervised Multi-View Gaze Representation Learning<br>
	<i>John Gideon, Shan Su, Simon Stent</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>
  </div>
</div><br>

   <br><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers</h2>

  <div class="paper">
        <span class="title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation</span>
        <span class="authors">Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</span>
        <span class="award">Best Paper Award</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Self-Attention with Convolution and Deconvolution for Efficient Eye Gaze Estimation from a Full Face Image</span>
        <span class="authors">Jun O Oh, Hyung Jin Chang, Sang-Il Choi</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/ANQ65NNNWNE"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Unsupervised Multi-View Gaze Representation Learning</span>
        <span class="authors">John Gideon, Shan Su, Simon Stent</span>
        <span class="award">Best Poster Award</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">ScanpathNet: A Recurrent Mixture Density Network for Scanpath Prediction</span>
        <span class="authors">Ryan Anthony J de Belen, Tomasz Bednarz, Arcot Sowmya</span>
        <span class="award">Best Paper Honourable Mention</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/de_Belen_ScanpathNet_A_Recurrent_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/8RXog3XkCl8"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">One-Stage Object Referring with Gaze Estimation</span>
        <span class="authors">Jianhang Chen, Xu Zhang, Yue Wu, Shalini Ghosh, Pradeep Natarajan, Shih-Fu Chang, Jan Allebach</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/SkjtCXX-aJY"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Characterizing Target-absent Human Attention</span>
        <span class="authors">Yupei Chen, Zhibo Yang, Souradeep Chakraborty, Sounak Mondal, Seoyoung Ahn, Dimitris Samaras, Minh Hoai, Gregory Zelinsky</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/SIVywYz2pNs"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings</span>
        <span class="authors">Anshul Gupta, Samy Tafasca, Jean-Marc Odobez</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://youtu.be/z-XSwLOpNzw"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>
  <br><br>

  <a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters</h2>

  <div class="paper">
        <span class="title">Dynamic 3D Gaze from Afar: Deep Gaze Estimation from Temporal Eye-Head-Body Coordination</span>
        <span class="authors">Soma Nonaka, Shohei Nobuhara, Ko Nishino</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22.pdf"><i class="fas fa-file-pdf"></i> PDF</a>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22_supp.pdf"><i class="fas fa-file-pdf"></i> Supp.</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/kyotovision-public/dynamic-3d-gaze-from-afar"><i class="fas fa-code"></i> Code</a>
	  <a class="btn btn-default" target="_blank" href="https://youtu.be/IEc8E4e4mXU"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>


  </div>
</div>
<br><br>

<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>


<div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://jiminpi.github.io/">Jimin Pi</a><h6>Hong Kong University of Science and Technology</h6></div>
    <div class="people-name"><a target="_blank" href="http://kaidierkes.net/?page_id=43">Kai Dierkes</a><h6>Pupil Labs Research</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.ccmitss.com/zhang">Xucong Zhang</a><h6>Delft University of Technology</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.perceptualui.org/people/wang/">Yao Wang</a><h6>Universität Stuttgart</h6></div>
    <div class="people-name"><a target="_blank" href="https://yihua.zone/">Yihua Cheng</a><h6>Beihuang University</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.mihaibace.com/">Mihai Bace</a><h6>University of Stuttgart</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://hengfei-wang.github.io//github.io/">Hengfei Wang</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.yusuke-sugano.info/">Yusuke Sugano</a><h6>The University of Tokyo</h6></div>
    <div class="people-name"><a target="_blank" href="https://ahnchive.github.io/">Seoyoung Ahn</a><h6>Stony Brook University</h6></div>
    <div class="people-name"><a target="_blank" href="">Jiabei Zeng</a><h6>Institute of Computing Technology, Chinese Academy of Sciences</h6></div>
    <div class="people-name"><a target="_blank" href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a><h6>NVIDIA Research</h6></div>
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/xiwang/">Xi Wang</a><h6>ETH Zurich</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://www.idiap.ch/~rsiegfried/">Rémy Siegfried</a><h6>Idiap Research Institute and EPFL</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.hci.uni-tuebingen.de/chair/team/wolfgang-fuhl">Wolfgang Fuhl</a><h6>University of Tuebingen</h6></div>
    <div class="people-name"><a target="_blank" href="https://kuangcy1998.wixsite.com/luckyspace">Chenyi Kuang</a><h6>Rensselaer Polytechnic Institute</h6></div>
    <div class="people-name"><a target="_blank" href="https://research.monash.edu/en/persons/shreya-ghosh">Shreya Ghosh</a><h6>Monash University</h6></div>
    <div class="people-name"><a target="_blank" href="https://ejcgt.github.io/">Eunji Chong</a><h6>Amazon</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://guohao.netlify.app/">Guohao Lan</a><h6>Delft University of Technology</h6></div>
    <div class="people-name"><a target="_blank" href="https://prashnani.github.io/">Ekta Prashnani</a><h6>University of California, Santa Barbara</h6></div>
    <div class="people-name"><a target="_blank" href="https://graphics.tudelft.nl/petr-kellnhofer/">Petr Kellnhofer</a><h6>TU Delft</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="">Jiawei Qin</a><h6>The University of Tokyo</h6></div>
  </div>
</div>
<br>

<br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "img/people/hj.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.ccmitss.com/zhang">
      <img class="people-pic" src="{{ "img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>
      <h6>Delft University of Technology</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>NVIDIA Research</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://thabobeeler.com/">
      <img class="people-pic" src="{{ "img/people/tb.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://thabobeeler.com/">Thabo Beeler</a>
      <h6>Google</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://www.ecse.rpi.edu/~qji/">
      <img class="people-pic" src="{{ "img/people/qj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ecse.rpi.edu/~qji/">Qiang Ji</a>
      <h6>Rensselaer Polytechnic Institute</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hengfei-wang.github.io//github.io/">
      <img class="people-pic" src="{{ "img/people/hengfei.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hengfei Wang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <br><br>
    Please contact me if you have any question about this website.
    <br>
    Email: hxw080@student.bham.ac.uk
</div>
<br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
</div> -->
