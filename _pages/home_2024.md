---
layout: layout_2024
urltitle:  "GAZE 2024: Gaze Estimation and Prediction in the Wild"
title: "GAZE 2024: Gaze Estimation and Prediction in the Wild"
categories: cvpr, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /2024/
favicon: /2024/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
# Nika_Akin
---

<br>
<div class="row">
  <div class="col-xs-12">
    <img class="img-fluid" src="{{ "img/banner.png" | prepend:site.baseurl }}">
    <small style="float:right;margin-top:0mm;margin-right:12mm;">Image credit to <a href="https://openai.com/dall-e-3" target="_blank">DALL·E</a></small>
    <br><br>
    <center>
      <table class="event-details">
        <tr><td class="item">Date:     </td><td class="desc">Tuesday, 18th June 2024      </td></tr>
        <tr><td class="item">Time:     </td><td class="desc">8:30 AM – 12:30 PM (half-day)</td></tr>
        <tr><td class="item">Location: </td><td class="desc">Arch 309                     </td></tr>
      </table>
    </center>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
    The 6th International Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2024) at <a href="https://cvpr.thecvf.com/Conferences/2024" target="_blank">CVPR 2024</a> aims to encourage and highlight novel strategies for eye gaze estimation and prediction. The workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Enhancing eye image segmentation, landmark localization, gaze estimation and other tasks in mixed and augmented reality (XR / AR) settings.</li>
      <li>Novel multi-modal systems for incorporating gaze information to improve visual recognition tasks.</li>
      <li>Improving eye detection, gaze estimation, and gaze prediction pipelines in various ways, such as by applying geometric and anatomical constraints, leveraging additional cues such as head pose, scene content, or considering multi-modal inputs.</li>
      <li>Developing adversarial or domain generalization methods to improve cross-dataset performance or to deal with conditions where current methods fail (illumination, appearance, etc.).</li>
      <li>Exploring attention mechanisms and temporal information to predict the point of regard.</li>
      <li>Novel methods for temporal gaze estimation and prediction including Bayesian methods.</li>
      <li>Personalization of gaze estimators with methods such as few-shot learning.</li>
      <li>Semi-/weak-/un-/self- supervised learning methods, domain adaptation methods, and other novel methods towards improved representation learning from eye/face region images or gaze target region images.</li>
    </ul>
    We will be hosting 2 invited speakers and will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself.
  </div>
</div> <br>

<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://github.com/cvpr-org/author-kit/releases" target="_blank">CVPR format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process.
	    All contributions must be submitted (along with supplementary materials, if any) at this <a href="https://cmt3.research.microsoft.com/GAZE2024/Submission/Index">CMT link</a>.
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
          </p>
        </div>
      </div>
    </div>
    <br>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 15, 2024 (23:59 Pacific time)</td>
	  <td><span class="countdown" reference="15 Mar 2024 23:59:59 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>April 5, 2024</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 14, 2024</td>
        </tr>
        <tr>
          <td>Workshop Day</td>
          <td>June 18, 2024</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
    <br>

    <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="http://xufeng.site/">
          <img class="people-pic" src="/2024/img/people/fx.jpg" />
        </a>
        <div class="people-name">
          <a href="http://xufeng.site/">Feng Xu</a>
          <h6>Tsinghua University</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Eye Region Reconstruction with a Monocular Camera</h3><br />
        <b>Abstract</b>
        <p class="speaker-abstract">Eye region reconstruction is an important yet challenging task in computer vision and graphics. It suffers from complicated geometry and motions, severe occlusions, and eyeglass interference, for which existing methods have to make a trade-off between capture cost and reconstruction quality. We focused on low-cost capture setups and proposed novel algorithms to achieve high-quality eye region reconstruction under limited inputs. In addition, we tried to solve the eyeglass interference, which lays the foundation for high-quality eye region reconstruction. We have also tried to apply eye region reconstruction in medicine for disease diagnosis.</p>

        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse">
	    <div class="panel-body">
              <p class="speaker-bio">
	        Feng Xu is currently an associate professor at the School of Software, Tsinghua University, Beijing, China. He earned a Ph.D. in automation and a B.S. in physics from Tsinghua University in 2012 and 2007, respectively. Until 2015, He was a Researcher in the Internet Graphics group, Microsoft Research Asia. His research interests include human body reconstruction, face animation, and medical image analysis. He has authored more than 40 conference and journal papers in the corresponding areas, including Nature Medicine, SIGGRAPH, CVPR, ICCV, ECCV, PAMI, and so on.
              </p>
	    </div>
          </div>
        </div>
      </div>
    </div>

    <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://about.meta.com/realitylabs/">
	  <img class="people-pic" src="/2024/img/people/af.jpg" />
	</a>
        <div class="people-name">
          <a href="https://about.meta.com/realitylabs/">Alexander Fix</a>
          <h6>Meta Reality Labs Research</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Challenges in Near Eye Tracking for AR/VR</h3><br />
        <b>Abstract</b>
        <p class="speaker-abstract">Artificial and Virtual Reality (AR/VR) has incredible potential for using eye tracking to power the future of computing, but also incredible challenges in making eye tracking that works for everyone, all the time. In this talk, I will talk about some of the work we’re doing here at Meta Reality Labs to build eye tracking into AR/VR, as well as the key areas where the CVPR and GAZE 2024 community can help solve the hardest problems in this space. We will highlight Aria – the ET-enabled research glasses from Meta – and how they are a great tool for investigating applications of eye tracking. We will also show some new approaches to doing eye tracking, based on event cameras, polarization, and more.</p>

        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse">
	    <div class="panel-body">
              <p class="speaker-bio">
	        Alexander Fix is a Research Scientist at Meta Reality Labs Research, where he has worked on eye tracking and related topics for the last 9 years. His research interests include 3D reconstruction, NeRF and other implicit reconstructions, and geometric eye tracking. Collaborations at Meta include quite a lot of eye tracking hardware research, particularly on novel methods for eye tracking such as Event Cameras. He graduated from Cornell in 2016 with a PhD in Computer Science, and from the University of Chicago in 2009 with a BS in Computer Science and Mathematics.
              </p>
	    </div>
          </div>
        </div>
      </div>
    </div>

  </div>
</div>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers</h2>

    <div class="paper">
      <span class="title">Spatio-Temporal Attention and Gaussian Processes for Personalized Video Gaze Estimation
</span>
      <span class="authors">Swati Jindal, Mohit Yadav, Roberto Manduchi</span>
      <!-- <span class="award">Best Paper Award</span> -->
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #8</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Jindal_Spatio-Temporal_Attention_and_Gaussian_Processes_for_Personalized_Video_Gaze_Estimation_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Jindal_Spatio-Temporal_Attention_and_CVPRW_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2404.05215"><i class="fas fa-archive"></i> arXiv</a>
        <!--<a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>-->
	 <a class="btn btn-default" target="_blank" href="https://github.com/jswati31/stage"><i class="fas fa-code"></i> Code</a>
      </div>
    </div>

    <div class="paper">
      <span class="title">Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following
</span>
      <span class="authors">Anshul Gupta, Pierre Vuillecard, Arya Farkhondeh, Jean-Marc Odobez</span>
      <!-- <span class="award">Best Paper Award</span> -->
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #9</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Gupta_Exploring_the_Zero-Shot_Capabilities_of_Vision-Language_Models_for_Improving_Gaze_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Gupta_Exploring_the_Zero-Shot_CVPRW_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <!--<a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>-->
        <!--<a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>-->
      </div>
    </div>

    <div class="paper">
      <span class="title">Gaze Scanpath Transformer: Predicting Visual Search Target by Spatiotemporal Semantic Modeling of Gaze Scanpath</span>
      <span class="authors">Takumi Nishiyasu, Yoichi Sato</span>
      <!-- <span class="award">Best Paper Award</span> -->
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #10</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Nishiyasu_Gaze_Scanpath_Transformer_Predicting_Visual_Search_Target_by_Spatiotemporal_Semantic_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <!--<a class="btn btn-default" target="_blank" href="CVF SUPPL URL"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
        <!--<a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>-->
        <!--<a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>-->
      </div>
    </div>

    <div class="paper">
      <span class="title">GESCAM: A Dataset and Method on Gaze Estimation for Classroom Attention Measurement</span>
      <span class="authors">Athul Mathew, Arshad Khan, Thariq Khalid, Riad Souissi</span>
      <!-- <span class="award">Best Paper Award</span> -->
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #11</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Mathew_GESCAM__A_Dataset_and_Method_on_Gaze_Estimation_for_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <!--<a class="btn btn-default" target="_blank" href="CVF SUPPL URL"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
	<a class="btn btn-default" target="_blank" href="https://athulmmathew.github.io/GESCAM/"><i class="fas fa-globe"></i> Project Page</a>
        <!--<a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>-->
        <!--<a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>-->
      </div>
    </div>

  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters</h2>

    <div class="paper">
      <span class="title">What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation</span>
      <span class="authors">Yihua Cheng, Yaning Zhu, Zongji Wang, Hongquan Hao, Liu Wei, Shiqing Cheng, Xi Wang, Hyung Jin Chang</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-primary">CVPR 2024</button>
	<button class="btn btn-poster-id">Poster #12</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_What_Do_You_See_in_Vehicle_Comprehensive_Vision_Solution_for_CVPR_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Cheng_What_Do_You_CVPR_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://yihua.zone/work/ivgaze/"><i class="fas fa-globe"></i> Project Page</a>
	<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2403.15664"><i class="fas fa-archive"></i> arXiv</a>
	<a class="btn btn-default" target="_blank" href="https://github.com/yihuacheng/IVGaze"><i class="fas fa-code"></i> Code</a>
        <!-- <a class="btn btn-default" target="_blank" href="SUPPL PDF"><i class="fas fa-file-pdf"></i> Supp.</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a> -->
      </div>
    </div>

    <div class="paper">
      <span class="title">Learning from Observer Gaze: Zero-shot Attention Prediction Oriented by Human-Object Interaction Recognition</span>
      <span class="authors">Yuchen Zhou, Linkai Liu, Chao Gou</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-primary">CVPR 2024</button>
	<button class="btn btn-poster-id">Poster #13</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Learning_from_Observer_Gaze_Zero-Shot_Attention_Prediction_Oriented_by_Human-Object_CVPR_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhou_Learning_from_Observer_CVPR_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <!--<a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>-->
        <!-- <a class="btn btn-default" target="_blank" href="SUPPL PDF"><i class="fas fa-file-pdf"></i> Supp.</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="GITHUB URL"><i class="fas fa-code"></i> Code</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a> -->
      </div>
    </div>

    <div class="paper">
      <span class="title">Sharingan: A Transformer Architecture for Multi-Person Gaze Following</span>
      <span class="authors">Samy Tafasca, Anshul Gupta, Jean-Marc Odobez</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-primary">CVPR 2024</button>
	<button class="btn btn-poster-id">Poster #14</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Tafasca_Sharingan_A_Transformer_Architecture_for_Multi-Person_Gaze_Following_CVPR_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Tafasca_Sharingan_A_Transformer_CVPR_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2310.00816"><i class="fas fa-archive"></i> arXiv</a>
        <!-- <a class="btn btn-default" target="_blank" href="SUPPL PDF"><i class="fas fa-file-pdf"></i> Supp.</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="GITHUB URL"><i class="fas fa-code"></i> Code</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a> -->
      </div>
    </div>

    <div class="paper">
      <span class="title">From Feature to Gaze: A Generalizable Replacement of Linear Layer for Gaze Estimation</span>
      <span class="authors">Yiwei Bao, Feng Lu</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-primary">CVPR 2024</button>
	<button class="btn btn-poster-id">Poster #15</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Bao_From_Feature_to_Gaze_A_Generalizable_Replacement_of_Linear_Layer_CVPR_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <!--<a class="btn btn-default" target="_blank" href="CVF SUPPL URL"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>-->
	<!--<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2310.00816"><i class="fas fa-archive"></i> arXiv</a>-->
        <!-- <a class="btn btn-default" target="_blank" href="SUPPL PDF"><i class="fas fa-file-pdf"></i> Supp.</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="GITHUB URL"><i class="fas fa-code"></i> Code</a> -->
        <!-- <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a> -->
      </div>
    </div>

  </div>
</div><br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <center><img class="people-pic" src="{{ "img/people/hj.png" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.ccmitss.com/zhang">
      <img class="people-pic" src="{{ "img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>
      <h6>Delft University of Technology</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>NVIDIA Research</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://thabobeeler.com/">
      <img class="people-pic" src="{{ "img/people/tb.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://thabobeeler.com/">Thabo Beeler</a>
      <h6>Google</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://swook.net/">
      <img class="people-pic" src="{{ "img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://swook.net/">Seonwook Park</a>
      <h6>NVIDIA Research</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
   <div class="col-xs-2">
    <a href="https://www.idiap.ch/~odobez/">
      <img class="people-pic" src="{{ "img/people/jmo.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.idiap.ch/~odobez/">Jean-Marc Odobez</a>
      <h6>EPFL &amp; Idiap Research Institute</h6>
    </div>
  </div>
   <div class="col-xs-2">
    <a href="https://yihua.zone/">
      <center><img class="people-pic" src="{{ "img/people/yc.jpg" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://yihua.zone/">Yihua Cheng</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://xiwang1212.github.io/homepage/">
      <center><img class="people-pic" src="{{ "img/people/xw.jpg" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://xiwang1212.github.io/homepage/">Xi Wang</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div>

<br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hengfei-wang.github.io/">
      <img class="people-pic" src="{{ "img/people/hengfei.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hengfei-wang.github.io/">Hengfei Wang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <br><br>
  <div class="col-xs-8">
    Please contact me if you have any question about this website.
    <br>
    Email: hxw080@student.bham.ac.uk
  </div>
  <div class="col-xs-1"></div>
</div>
<br>


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.birmingham.ac.uk/"><img src="img/uob.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
</div>
