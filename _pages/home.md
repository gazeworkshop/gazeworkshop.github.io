---
layout: project
urltitle:  "3D Scene Genneration"
title: "3D Scene Generation"
categories: cvpr, workshop, computer vision, computer graphics, visual learning, simulation environments, robotics, machine learning, natural language processing, reinforcement learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>3D Scene Generation</h1></center>
    <center><h2>CVPR 2019 Workshop, Long Beach, CA</h2></center>
    <!-- <center><span style="color:#e74c3c;font-weight:400;">Time and location TBA</span></center> -->
    <center>Sunday June 16 2019, 8:45am -- 5:40pm, <span style="color:#e74c3c;font-weight:400;">location TBA</span></center>
  </div>
</div>

<hr>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      People spend a large percentage of their lives indoors---in bedrooms, living rooms, offices, kitchens, and other such spaces---and the demand for virtual versions of these real-world spaces has never been higher. Game developers, VR/AR designers, architects, and interior design firms are all increasingly making use virtual 3D scenes for prototyping and final products. Furthermore, AI/vision/robotics researchers are also turning to virtual environments to train data-hungry models for tasks such as visual navigation, 3D reconstruction, activity recognition, and more.
    </p>
    <p>
      As the vision community turns from passive internet-images-based vision tasks to applications such as the ones listed above, the need for virtual 3D environments becomes critical. The community has recently benefited from large scale datasets of both synthetic 3D environments [13] and reconstructions of real spaces [8, 9, 14, 16], and the development of 3D simulation frameworks for studying embodied agents [3, 10, 11, 15]. While these existing datasets are a valuable resource, they are also finite in size and don't adapt to the needs of different vision tasks. To enable large-scale embodied visual learning in 3D environments, we must go beyond such static datasets and instead pursue the automatic synthesis of novel, task-relevant virtual environments.
    </p>
    <p>
      In this workshop, we aim to bring together researchers working on automatic generation of 3D environments for computer vision research with researchers who are making use of 3D environment data for a variety of computer vision tasks. We define "generation of 3D environments" to include methods that generate 3D scenes from sensory inputs (e.g. images) or from high-level specifications (e.g. "a chic apartment for two people"). Vision tasks that consume such data include automatic scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, and more.
    </p>
  </div>
</div> <br>   

<div class="row">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite extended abstracts for work on tasks related to 3D scene generation or tasks leveraging generated 3D scenes.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D scene synthesis</li>
      <li>Synthesis of 3D scenes from sensor inputs (e.g., images, videos, or scans)</li>
      <li>Representations for 3D scenes</li>
      <li>3D scene understanding based on synthetic 3D scene data</li>
      <li>Completion of 3D scenes or objects in 3D scenes</li>
      <li>Learning from real world data for improved models of virtual worlds</li>
      <li>Use of 3D scenes for simulation targeted to learning in computer vision, robotics, and cognitive science</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 6 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted extended abstracts will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We also welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>
  </div>
</div><br>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>May 17 2019</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 31 2019</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>June 7 2019</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>June 17 2019</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 1</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 2</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:10am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:10am - 11:10am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 3</td>
          <td>11:10am - 11:35am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 4</td>
          <td>11:35am - 12:00pm</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>12:00pm - 1:30pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 5 (Industry Talks)</td>
          <td>1:30pm - 2:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 6</td>
          <td>2:00pm - 2:25pm</td>
        </tr>
        <tr>
          <td>Oral 1</td>
          <td>2:25pm - 2:45pm</td>
        </tr>
        <tr>
          <td>Oral 2</td>
          <td>2:45pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 7</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 8</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>4:50pm - 5:40pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<br>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://vladlen.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/vladlen.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://vladlen.info/">Vladlen Koltun</a></b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. Previously, he has been a Senior Research Scientist at Adobe Research and an Assistant Professor at Stanford where his theoretical research was recognized with the National Science Foundation (NSF) CAREER Award (2006) and the Sloan Research Fellowship (2007).
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://www.cs.utexas.edu/users/grauman/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/grauman.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin and a Research Scientist in Facebook AI Research (FAIR).  Her research in computer vision and machine learning focuses on visual recognition and search.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She is an Alfred P. Sloan Research Fellow and Microsoft Research New Faculty Fellow, a recipient of NSF CAREER and ONR Young Investigator awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the International Joint Conference on Artificial Intelligence (IJCAI), the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013, and the Helmholtz Prize in 2017. 
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.inf.ethz.ch/personal/marc.pollefeys/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/pollefeys.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a></b> is a full professor and head of the Institute for Visual Computing of the Dept. of Computer Science of ETH Zurich which he joined in 2007.  He leads the Computer Vision and Geometry lab.  Previously he was with the Dept. of Computer Science of the University of North Carolina at Chapel Hill where he started as an assistant professor in 2002 and became an associate professor in 2005.  Before he was a postdoctoral researcher at the Katholieke Universiteit Leuven in Belgium, where he also received his M.S. and Ph.D. degrees in 1994 and 1999, respectively. His main area of research is computer vision.  One of his main research goals is to develop flexible approaches to capture visual representations of real world objects, scenes and events. Dr. Pollefeys has received several prizes for his research, including a Marr prize, an NSF CAREER award, a Packard Fellowship and a ERC Starting Grant. He is the author or co-author of more than 280 peer-reviewed papers.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://cs.brown.edu/people/epavlick/index.html"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/ellie.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://cs.brown.edu/people/epavlick/index.html">Ellie Pavlick</a></b> is an Assistant Professor of Computer Science at Brown University, and an academic partner with Google AI. She received her PhD in Computer Science from the University of Pennsylvania. She is interested in building better computational models of natural language semantics and pragmatics: how does language work, and how can we get computers to understand it the way humans do?
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.purdue.edu/homes/aliaga/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/aliaga.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a></b> does research primarily in the area of 3D computer graphics but overlaps with computer vision and visualization while also having strong multi-disciplinary collaborations outside of computer science. His research activities are divided into three groups: a) his pioneering work in the multi-disciplinary area of inverse modeling and design; b) his first-of-its-kind work in codifying information into images and surfaces, and c) his compelling work in a visual computing framework including high-quality 3D acquisition methods. Dr. Aliaga’s inverse modeling and design is particularly focused at digital city planning applications that provide innovative “what-if” design tools enabling urban stake holders from cities worldwide to automatically integrate, process, analyze, and visualize the complex interdependencies between the urban form, function, and the natural environment.
    </p>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://www.cse.iitb.ac.in/~sidch/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/sid.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a></b> is a Senior Research Scientist at Adobe Research, and Assistant Professor (on leave) of Computer Science and Engineering at IIT Bombay. His research focuses on richer tools for designing three-dimensional objects, particularly by novice and casual users, and on related problems in 3D shape understanding, synthesis and reconstruction. He received his PhD from Stanford University, followed by a postdoc at Princeton and a year teaching at Cornell. Apart from basic research, he is also the original author of the commercial 3D modeling package Adobe Fuse.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="http://graphics.stanford.edu/~adai/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/angela.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://graphics.stanford.edu/~adai/">Angela Dai</a></b> is a postdoctoral researcher at the Technical University of Munich.  She received her Ph.D. in Computer Science at Stanford University advised by Pat Hanrahan. Her research focuses on 3D reconstruction and understanding with commodity sensors. She received her Masters degree from Stanford University and her Bachelors degree from Princeton University. She is a recipient of a Stanford Graduate Fellowship.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/jiajun.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is a fifth-year PhD student at MIT, advised by Bill Freeman and Josh Tenenbaum. He received his undergraduate degree from Tsinghua University, working with Zhuowen Tu. He has also spent time at research labs of Microsoft, Facebook, and Baidu. His research has been supported by fellowships from Facebook, Nvidia, Samsung, Baidu, and Adobe. He studies machine perception, reasoning, and its interaction with the physical world, drawing inspiration from human cognition.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <b>Industry Participants</b>
    <p>The workshop also features presentations by representatives of the following companies:</p>
  </div>
</div>
<div class="row">
  <div class="col-md-3">
    <a href="https://planner5d.com/"><img src="/static/img/p5d.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://wayfair.com/"><img src="/static/img/wayfair.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://modsy.com/"><img src="/static/img/modsy.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://zillow.com/"><img src="/static/img/zillow.png" /></a>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Eloquent Labs, Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Facebook AI Research, Simon Fraser University</h6>
    </div>
  </div>
</div>

<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<!-- <div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div> -->

{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}  
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  

{:.paper}
<span>[2] GRAINS: Generative Recursive Autoencoders for INdoor Scenes</span>{:.papertitle}  
<span>M. Li, A.G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. Zhang</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1807.09193, 2018</span>{:.journal}  

{:.paper}
<span>[3] Gibson env: real-world perception for embodied agents</span>{:.papertitle}  
<span>F. Xia, A. R. Zamir, Z.Y. He, A. Sax, J. Malik, and S. Savarese</span>{:.authors}  
<span>Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on, IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[4] VirtualHome: Simulating Household Activities via Programs</span>{:.papertitle}  
<span>X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[5] Embodied Question Answering</span>{:.papertitle}  
<span>A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[6] ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</span>{:.papertitle}  
<span>A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[7] SeeThrough: Finding Objects in Heavily Occluded Indoor Scene Images</span>{:.papertitle}  
<span>N. Mitra, V. Kim, E. Yumer, M. Hueting, N. Carr, and P. Reddy</span>{:.authors}  
<span>2018 International Conference on 3D Vision (3DV), 2018</span>{:.journal}  

{:.paper}
<span>[8] Matterport3D: Learning from RGB-D Data in Indoor Environments</span>{:.papertitle}  
<span>A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang</span>{:.authors}  
<span>_International Conference on 3D Vision (3DV)_, 2017</span>{:.journal}  

{:.paper}
<span>[9] Joint 2D-3D-semantic data for indoor scene understanding</span>{:.papertitle}  
<span>I. Armeni, S. Sax, A.R. Zamir, and S. Savarese</span>{:.authors}  
<span>_arXiv preprint arXiv:1702.01105_, 2017</span>{:.journal}  

{:.paper}
<span>[10] MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</span>{:.papertitle}  
<span>M. Savva, A.X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun</span>{:.authors}  
<span>_arXiv:1712.03931_, 2017</span>{:.journal}  

{:.paper}
<span>[11] AI2-THOR: An interactive 3D environment for visual AI</span>{:.papertitle}  
<span>E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi</span>{:.authors}  
<span>_arXiv preprint arXiv:1712.05474_, 2017</span>{:.journal}  

{:.paper}
<span>[12] Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</span>{:.papertitle}  
<span>Y. Zhang, S. Song, E. Yumer, M. Savva, J.Y. Lee, H. Jin, and T. Funkhouser</span>{:.authors}  
<span>_The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017</span>{:.journal}  

{:.paper}
<span>[13] Semantic scene completion from a single depth image</span>{:.papertitle}  
<span>S. Song, F. Yu, A. Zeng, A.X. Chang, M. Savva, and T. Funkhouser</span>{:.authors}  
<span>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017</span>{:.journal}  

{:.paper}
<span>[14] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</span>{:.papertitle}  
<span>A. Dai, A.X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017</span>{:.journal}  

{:.paper}
<span>[15]  CARLA: An Open Urban Driving Simulator</span>{:.papertitle}  
<span>A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun</span>{:.authors}  
<span>1--16, Proceedings of the 1st Annual Conference on Robot Learning, 2017</span>{:.journal}  

{:.paper}
<span>[16] SceneNN: A Scene Meshes Dataset with aNNotations</span>{:.papertitle}  
<span>B.S. Hua, Q.H. Pham, D.T. Nguyen, M.K. Tran, L.F. Yu, and S.K. Yeung</span>{:.authors}  
<span>International Conference on 3D Vision (3DV), 2016</span>{:.journal}  

