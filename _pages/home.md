---
layout: project
urltitle:  "Gaze Estimation and Prediction in the Wild"
title: "Gaze Estimation and Prediction in the Wild"
categories: iccv, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /
favicon: /static/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Gaze Estimation and Prediction in the Wild</h1></center>
    <center><h2>ICCV 2019 Workshop, Seoul, Korea</h2></center>
    <!-- <center><span style="color:#e74c3c;font-weight:400;">Time and location TBA</span></center>
    <center>TODO - DATE Sunday June 16 2019, 8:45am -- 5:40pm, <span style="color:#e74c3c;font-weight:400;"> TODO -location TBA</span></center>-->
  </div>
</div>

<hr>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="{{ "/static/img/Seoul.jpg" | prepend:site.baseurl }}">
    <p align="right"> Photo: Shutterstock</p>
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 1st Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2019) at ICCV 2019 is the first-of-its-kind workshop focused on designing and evaluating deep learning methods for the task of gaze estimation and prediction. We aim to encourage and highlight novel strategies with a focus on robustness and accuracy in real-world settings. This is expected to be achieved via novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training among other directions. This half-day workshop consists of three invited talks as well as lightning talks from industry contributors.
    </p>
  </div>
</div> <br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page ICCV format) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted through CMT in the following link: <a href="https://cmt3.research.microsoft.com/GAZE2019" target="_blank" title="CMT Submission System for GAZE 2019">https://cmt3.research.microsoft.com/GAZE2019</a>.
    </p>
    <p>
      The topics of this workshop include but are not limited to:
    </p>
    <ul>
      <li>Proposal of novel eye detection, gaze estimation, and gaze prediction pipelines using deep convolutional neural networks.</li>
      <li>Incorporating geometric and anatomical constraints into neural networks in a differentiable manner.</li>
      <!--<li>Exploring attention mechanisms to improve the estimation or prediction of users’ points of regard.</li>-->
      <li>Demonstration of robustness to conditions where current methods fail (illumination, appearance, low-resolution etc.).</li>
      <li>Robust estimation from different data modalities such as RGB, depth, and near infra-red.<!--, head pose, and eye region landmarks--></li>
      <li>Leveraging additional cues such as task context, temporal information, eye movement classification.</li>
      <li>Designing new accurate metrics to account for rapid eye movements in the real world.</li>
      <li>Semi-supervised / unsupervised / self-supervised learning, meta-learning, domain adaptation, attention mechanisms and other related machine learning methods for gaze estimation.</li>
      <li>Methods for temporal gaze estimation and prediction including bayesian methods.</li>
    </ul>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12">
    <h2>Call for Extended Abstracts</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ICCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.
    </p>
    <p>Extended abstracts are limited to three pages and must be created using <a href="/static/GAZE 2019 Extended Abstract Template.zip" title="GAZE 2019 Extended Abstracts Template">this LaTeX template</a>. The submission must be sent to <a href="mailto:gaze.iccv19@gmail.com">gaze.iccv19@gmail.com</a> by 6th September.
    </p>
    <p>
      We will evaluate and notify authors of acceptance as soon as possible after receiving their extended abstract submission.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12">
    <h2>Call for Accepted ICCV/CVPR Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Accepted papers at the main conference (ICCV 2019) or papers which were presented at CVPR 2019 are welcome to be presented during our poster session to increase the exposure of your work and foster discussion in the community. Please send a PDF document of your camera-ready paper to <a href="mailto:gaze.iccv19@gmail.com">gaze.iccv19@gmail.com</a> at any time to register your presence.
    </p>
  </div>
</div><br>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>July 29, 2019 (23:59 Pacific time)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>August 16, 2019</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>August 30, 2019</td>
        </tr>
        <tr>
          <td>Extended Abstracts Deadline</td>
          <td>September 6, 2019</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>October 27, 2019 (Morning)</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<!--div class="row" >
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<p><br /></p>
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 1</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 2</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:10am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:10am - 11:10am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 3</td>
          <td>11:10am - 11:35am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 4</td>
          <td>11:35am - 12:00pm</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>12:00pm - 1:30pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 5 (Industry Talks)</td>
          <td>1:30pm - 2:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 6</td>
          <td>2:00pm - 2:25pm</td>
        </tr>
        <tr>
          <td>Oral 1</td>
          <td>2:25pm - 2:45pm</td>
        </tr>
        <tr>
          <td>Oral 2</td>
          <td>2:45pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 7</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 8</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>4:50pm - 5:40pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>-->

<!--br>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<p><br /></p>
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>-->


<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<div class="row speaker">
  <div class="col-xs-3 speaker-pic">
    <a href="https://perceptualui.org/people/bulling/">
      <img class="people-pic" src="{{ "/static/img/people/ab.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://perceptualui.org/people/bulling/">Andreas Bulling</a>
      <h6>University of Stuttgart</h6>
    </div>
  </div>
  <div class="col-xs-9">
    <b>Biography</b>
    <p class="speaker-bio">
Andreas Bulling is Full Professor (W3) of Computer Science at the University of Stuttgart where he holds the chair for Human-Computer Interaction and Cognitive Systems. He is also Faculty at the International Max Planck Research School for Intelligent Systems (IMPRS-IS), Member in the Cluster of Excellence "Data-integrated Simulation Science" (SimTech), and partner of Pupil Labs GmbH. He received his MSc. (Dipl.-Inform.) in Computer Science from the Karlsruhe Institute of Technology (KIT), Germany, focusing on embedded systems, robotics, and biomedical engineering. He holds a PhD in Information Technology and Electrical Engineering from the Swiss Federal Institute of Technology (ETH) Zurich, Switzerland. Andreas was previously a Feodor Lynen Research Fellow and a Marie Curie Research Fellow in the Computer Laboratory at the University of Cambridge, UK, a postdoctoral research associate in the School of Computing and Communications at Lancaster University, UK, as well as a Junior Research Fellow at Wolfson College, Cambridge. From 2013 – 2018 he was a Senior Researcher at the Max Planck Institute for Informatics and an Independent Research Group Leader (W2) at the Cluster of Excellence on Multimodal Computing and Interaction (MMCI) at Saarland University. Andreas is UbiComp steering committee member and serves on the editorial boards of the Proceedings of the ACM on Interactive, Mobile, Wearable, and Ubiquitous Technologies, ACM Transactions on Interactive Intelligent Systems, and the Journal of Eye Movement Research. He served as co-chair, TPC member, and reviewer for major conferences, most recently as TPC co-chair for ACM UbiComp 2016 and IEEE PerCom 2015, associate chair for ACM ETRA 2016 and 2018 as well as ACM CHI 2013, 2014, 2018, and 2019, and general chair for ETRA 2020. He received an ERC Starting Grant in 2018.
    </p>
  </div>
</div>
<div class="row speaker">
  <div class="col-xs-3 speaker-pic">
    <a href="https://www.yusuke-sugano.info/">
      <img class="people-pic" src="{{ "/static/img/people/ys.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.yusuke-sugano.info/">Yusuke Sugano</a>
      <h6>University of Tokyo</h6>
    </div>
  </div>
  <div class="col-xs-9">
    <b>Biography</b>
    <p class="speaker-bio">
Yusuke Sugano is an associate professor at Institute of Industrial Science, The University of Tokyo. His research interests focus on computer vision and human-computer interaction. He received his Ph.D. in information science and technology from the University of Tokyo in 2010. He was previously an associate professor at Graduate School of Information Science and Technology, Osaka University, a postdoctoral researcher at Max Planck Institute for Informatics, and a project research associate at Institute of Industrial Science, the University of Tokyo.
    </p>
  </div>
</div>
<br>


<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "/static/img/people/hj.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "/static/img/people/sp.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/zhang/">
      <img class="people-pic" src="{{ "/static/img/people/xz.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "/static/img/people/oh.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "/static/img/people/al.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div>
<br>


<div class="row" >
  <div class="col-xs-12">
    <h2>Workshop sponsored by:</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-3">
    <a href="https://www.samsung.com/"><img src="/static/img/samsung.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://www.nvidia.com/"><img src="/static/img/nvidia.jpg" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://www.tobii.com/"><img src="/static/img/tobii.jpg" /></a>
  </div>
</div>


<hr>
<br>

<!--div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>-->


<!--{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  -->

