---
layout: layout_2019
urltitle:  "GAZE 2019: Gaze Estimation and Prediction in the Wild"
title: "GAZE 2019: Gaze Estimation and Prediction in the Wild"
categories: iccv, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /2019/
favicon: /2019/img/icon.jpg
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Gaze Estimation and Prediction in the Wild</h1></center>
    <center><h2>ICCV 2019 Workshop, Seoul, Korea</h2></center>
    <center>Sunday October 27 2019, 8:30am - 12:50pm</center>
    <center>Location: <b>Room 318 A</b></center>
  </div>
</div>

<hr>
<div class="row">
  <div class="col-md-12">
    <div class="alert alert-success">
      Please <b><a href="https://groups.google.com/forum/#!forum/gaze-in-the-wild" target="_blank">subscribe</a></b> to our
	  <b><a href="https://groups.google.com/forum/#!forum/gaze-in-the-wild" target="_blank">new mailing list</a></b>
	  to gain access to our speakers' slides and future updates!
    </div>
  </div>
</div>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="{{ "img/Seoul.jpg" | prepend:site.baseurl }}">
    <p align="right"> Photo: Shutterstock</p>
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 1st Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2019) at <a href="http://iccv2019.thecvf.com/program/workshops" target="_blank">ICCV 2019</a> is the first-of-its-kind workshop focused on designing and evaluating deep learning methods for the task of gaze estimation and prediction. We aim to encourage and highlight novel strategies with a focus on robustness and accuracy in real-world settings. This is expected to be achieved via novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training among other directions. This half-day workshop consists of three invited talks as well as talks from industry contributors.
    </p>
    <p>
      The topics of this workshop include but are not limited to:
    </p>
    <ul>
      <li>Proposal of novel eye detection, gaze estimation, and gaze prediction pipelines using deep convolutional neural networks.</li>
      <li>Incorporating geometric and anatomical constraints into neural networks in a differentiable manner.</li>
      <!--<li>Exploring attention mechanisms to improve the estimation or prediction of usersâ€™ points of regard.</li>-->
      <li>Demonstration of robustness to conditions where current methods fail (illumination, appearance, low-resolution etc.).</li>
      <li>Robust estimation from different data modalities such as RGB, depth, and near infra-red.<!--, head pose, and eye region landmarks--></li>
      <li>Leveraging additional cues such as task context, temporal information, eye movement classification.</li>
      <li>Designing new accurate metrics to account for rapid eye movements in the real world.</li>
      <li>Semi-supervised / unsupervised / self-supervised learning, meta-learning, domain adaptation, attention mechanisms and other related machine learning methods for gaze estimation.</li>
      <li>Methods for temporal gaze estimation and prediction including bayesian methods.</li>
    </ul>
  </div>
</div> <br>

<div class="row" id="call">
  <div class="col-xs-12 panel-group">
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page ICCV format) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted through CMT in the following link: <a href="https://cmt3.research.microsoft.com/GAZE2019" target="_blank" title="CMT Submission System for GAZE 2019">https://cmt3.research.microsoft.com/GAZE2019</a>.
          </p>
        </div>
      </div>
    </div>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-ea" style="cursor:pointer;">
        <h3 style="margin:0;">Extended Abstracts</h3>
      </div>
      <div id="call-ea" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
            In addition to regular papers, we also invite extended abstracts of ongoing or published work (<i>e.g.</i> related papers on ICCV main track). The extended abstracts will not be published or made available to the public (we will only list titles on our website) but will rather be presented during our poster session. We see this as an opportunity for authors to promote their work to an interested audience to gather valuable feedback.
          </p>
          <p>Extended abstracts are limited to three pages and must be created using <a href="GAZE 2019 Extended Abstract Template.zip" title="GAZE 2019 Extended Abstracts Template">this LaTeX template</a>. The submission must be sent to <a href="mailto:gaze.iccv19@gmail.com">gaze.iccv19@gmail.com</a> by 16th September.
          </p>
          <p>
            We will evaluate and notify authors of acceptance as soon as possible after receiving their extended abstract submission.
          </p>
        </div>
      </div>
    </div>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-poster" style="cursor:pointer;">
        <h3 style="margin:0;">Accepted ICCV/CVPR Papers</h3>
      </div>
      <div id="call-poster" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          Relevant papers that were accepted at the main conference (ICCV 2019) or at CVPR 2019 are welcome to be presented during our poster session to increase the exposure of your work and foster discussion in the community. Please send a PDF document of your camera-ready paper to <a href="mailto:gaze.iccv19@gmail.com">gaze.iccv19@gmail.com</a> at any time to register your presence.
        </div>
      </div>
    </div>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td><s style="color:darkred;">July 29</s> July 31, 2019 (23:59 Pacific time)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td><s style="color:darkred;">August 16</s> August 18, 2019</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>August 30, 2019</td>
        </tr>
        <tr id="schedule">
          <td>Extended Abstracts Deadline</td>
          <td><s style="color:darkred;">September 6</s> September 16, 2019</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>October 27, 2019 (Morning)</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
     <h2>Workshop Schedule</h2>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>#</th>
		  <th>Time</th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
	  <td>1</td>
		  <td>8:30am - 8:35am</td>
          <td>Welcome and Opening Remarks</td>
        </tr>
        <tr>
	  <td>2</td>
		  <td>8:35am - 10:05am</td>
          <td>Keynote Talks </td>
        </tr>
        <tr class="noline">
	  <td></td>
	      <td></td>
		  <!--<td>8:35am - 9:20am</td>-->
          <td><a href="#yusuke">Yusuke Sugano</a> (University of Tokyo)
		  </td>
        </tr>
        <tr class="noline">
	  <td></td>
	      <td></td>
		  <!--<td>9:20am - 10:05am</td>-->
          <td><a href="#jeanmarc">Jean-Marc Odobez</a> (Idiap and EPFL)</td>
        </tr>
        <tr>
	  <td>3</td>
		  <td>10:05am - 10:20am</td>
          <td>Accepted Full Paper Lightning Talks</td>
        </tr>
        <tr>
	  <td>4</td>
		  <td>10:20am - 11:00am</td>
          <td>Coffee Break and Poster Session</td>
        </tr>
        <tr>
	  <td>5</td>
		  <td>11:00am - 12:15pm</td>
          <td>Industry Keynote Talks </td>
        </tr>
        <tr class="noline">
	  <td></td>
	  	  <td></td>
		  <!--<td>11:00am - 11:25am</td>-->
          <td><a href="#jaejoon">Jae-Joon Han</a> (Samsung Advanced Institute of Technology)</td>
        </tr>
        <tr class="noline">
	  <td></td>
	  	  <td></td>
		  <!--<td>11:25am - 11:50pm</td>-->
          <td><a href="#shalini">Shalini De Mello</a> (Nvidia)</td>
        </tr>
        <tr class="noline">
	  <td></td>
	  	  <td></td>
		  <!--<td>11:50pm - 12:15pm</td>-->
          <td><a href="#maria">Maria Gordon</a> (Tobii)</td>
        </tr>
        <tr id="speakers">
	  <td>6</td>
		  <td>12:15pm - 12:40pm</td>
          <td>Panel Discussion</td>
        </tr>
        <tr>
	  <td>7</td>
		  <td>12:40pm - 12:50pm</td>
          <td>Presentation of Awards and Closing Remarks</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!--<br>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>


  </div>
</div>
-->

<br>


<div class="row">
  <div class="col-xs-12">
    <h2>Invited Keynote Speakers</h2>
  </div>
</div>

<div class="row speaker" id="yusuke">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.yusuke-sugano.info/">
      <img class="people-pic" src="{{ "img/people/ys_2.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.yusuke-sugano.info/">Yusuke Sugano</a>
      <h6>University of Tokyo</h6>
    </div>
  </div>
  <div class="col-md-9">
    <h3>Appearance-based Gaze Estimation: What We Have Done and What We Should Do</h3>
    <br>
    <b>Abstract</b>
    <p class="speaker-abstract">
    Since its first appearance in the 90s, appearance-based gaze
    estimation has been gradually but steadily gaining attention until
    now. This talk aims at providing a brief overview of past research
    achievements in the area of appearance-based gaze estimation, mainly
    from the perspective of both personalization and generalization
    techniques. I will also discuss some remaining challenges towards the
    ultimate goal of camera-based versatile gaze estimation techniques.
    </p>
    <b>Biography</b>
    <p class="speaker-bio">
Yusuke Sugano is an associate professor at Institute of Industrial Science, The University of Tokyo. His research interests focus on computer vision and human-computer interaction. He received his Ph.D. in information science and technology from the University of Tokyo in 2010. He was previously an associate professor at Graduate School of Information Science and Technology, Osaka University, a postdoctoral researcher at Max Planck Institute for Informatics, and a project research associate at Institute of Industrial Science, the University of Tokyo.
    </p>
  </div>
</div>
<br>


<div class="row speaker" id="jeanmarc">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.idiap.ch/~odobez/">
      <img class="people-pic" src="{{ "img/people/jmo.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.idiap.ch/~odobez/">Jean-Marc Odobez</a>
      <h6>Idiap Research Institute <br>and EPFL</h6>
    </div>
  </div>
  <div class="col-md-9">
    <h3>Measuring attention in  interactions: from context based multimodal head pose analysis to 3D gaze estimation</h3>
    <br>
    <b>Abstract</b>
    <p class="speaker-abstract">
Beyond words, non-verbal behaviors (NVB) are known to play important roles in face-to-face interactions. However, decoding non-verbal behaviors is a challenging problem that involves both extracting subtle physical NVB cues and mapping them to higher-level communication behaviors or social constructs. This is particularly the case of gaze, one of the most important non-verbal behaviors with functions related to communication and social signaling.
<br>
<br>
In this talk, I will present our past and current work towards the automatic analysis of attention (whether 3D gaze or its discrete version the Visual Focus of Attention, VFOA)  in situations where large user mobility is expected and minimal intrusion is required. I will first introduce how we addressed VFOA recognition in meetings using Dynamical Bayesian Networks to jointly model speech conversation, gaze (represented by head pose), and task context. I will then present recent methods investigated to perform 3D gaze tracking, including robust and accurate 3D head pose tracking under 360 degrees as well as the use of several deep neural network architectures for  appearance-based gaze estimation. The latter will include methods to build personalized models through few-shot learning and gaze redirection eye synthesis, differential gaze estimation, and online learning or adaptation, potentially taking advantage of priors on social interactions to obtain weak labels for model adaptation.
    </p>
    <b>Biography</b>
    <p class="speaker-bio">
    Dr. Jean-Marc Odobez received his PhD from Rennes University/INRIA in 1994 and was, from 1996 to 2001, Assistant Professor at the University of Maine, France. He is now a Senior Researcher at Idiap and adjunct faculty at the Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL) where he is a member of the School of Engineering (STI).
    </p>
    <p class="speaker-bio">
    He is the author or coauthor of more than 150 papers, and has been the principal investigator of more than 14 European and Swiss projects. He holds several patents in computer vision, and is the cofounder of the companies <a href="http://www.klewel.com" target="_blank">Klewel SA</a> and <a href="http://www.eyeware.tech/" target="_blank">Eyeware SA</a> companies. He is a member of the IEEE, and Associate Editor of the IEEE Transaction on Circuits and Systems for Video Technology and of Machine Vision and Application journals.
    </p>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>Invited Industry Speakers</h2>
  </div>
</div>

<div class="row speaker" id="jaejoon">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.researchgate.net/profile/Jae_Joon_Han2">
      <img class="people-pic" src="{{ "img/people/jjh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.researchgate.net/profile/Jae_Joon_Han2">Jae-Joon Han</a>
      <h6>Samsung Advanced Institute of Technology</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Jae-Joon Han is a Master of AI & SW Research Center at <a href="https://www.sait.samsung.co.kr" target="_blank">Samsung Advanced Institute of Technology (SAIT)</a>, the corporate research of Samsung Electronics. He received Ph.D degree in Electrical and computer engineering from Purdue University in 2006 and did a postdoctoral fellow at Purdue before he joined SAIT in 2007. Since then, he has mainly focused on developing computer vision and machine learning algorithms which enable for users to interact with devices in a novel way. He is currently leading a project related to on-device facial recognition. His research interest includes facial recognition, facial anti-spoofing, speaker verification, neural network model compression for on-device processing and gaze estimation.
    </p>
  </div>
</div>
<br>

<div class="row speaker" id="shalini">
  <div class="col-sm-3 speaker-pic">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Shalini De Mello is a Principal Research Scientist at NVIDIA. Her research interests are in computer vision and machine learning for human-computer interaction and smart interfaces. At NVIDIA, she has invented technologies for gaze estimation, and 2D and 3D head pose estimation, hand gesture recognition, face detection, video stabilization and GPU-optimized libraries for mobile computer vision. Her research over that past several years has pushed the envelope of HCI in cars and has led to the development of NVIDIAâ€™s innovative <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-ix/" target="_blank">DriveIX</a> product for smart <a href="https://www.youtube.com/watch?v=EVymqG9mdJg" target="_blank">AI-based automotive interfaces</a> for future generations of cars. She received doctoral and masterâ€™s degrees in Electrical and Computer Engineering from the University of Texas at Austin in 2008 and 2004, respectively.
    </p>
  </div>
</div>
<br>

<div class="row speaker" id="maria">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.linkedin.com/in/gordonmaria">
      <img class="people-pic" src="{{ "img/people/mg.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/gordonmaria">Maria Gordon</a>
      <h6>Tobii</h6>
    </div>
  </div>
  <div class="col-md-9">
    <h3>Gaze into the future</h3>
    <br>
    <b>Abstract</b>
    <p class="speaker-abstract">
Just a few years ago, eye tracking was still considered a research tool and a niche product used for e.g. offering people with assistive needs a voice. Today, however, eye tracking is broadcasted on major sports gaming tournaments and has become integrated into commercial off-the-shelf PCs and VR headsets.
<br>
A major pull for eye tracking technology today comes from the XR space. While eye tracking already now adds value to current XR products, future XR hardware, such as e.g. dynamic focus displays, simply wonâ€™t work without refined eye tracking solutions. In this talk weâ€™ll describe what role eye tracking will play in future remote and near eye setups and where eye tracking simply needs to succeed.
    </p>
    <b>Biography</b>
    <p class="speaker-bio">Maria has been working with software and algorithm development for more than 15 years. During her career she has held a variety of roles from developer to research lead at companies like Philips, Infineon and Saab. Maria joined Tobiiâ€™s eye tracking algorithm team in 2015 as a Key Algorithm Engineer.
    </p>
    <p class="speaker-bio" id="awards">An eye tracker that works for every single individual is still a challenge. Maria has experienced from close how PC eye tracking has moved from being a specialty market and research tool to a mainstream gaming equipment. She has also been closely involved in the development of eye tracking for VR and AR, and been part of the work behind releasing products such as the <a href="https://vr.tobii.com/products/htc-vive-pro-eye/" target="_blank">HTC Vive Pro Eye</a>.
    </p>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12" id="accepted">
    <h2>Awards</h2>
    <div class="award">
      <h3>
	    Best Paper Award
	    <span class="award-sponsor">sponsored by
        <a href="https://www.samsung.com/"><img src="img/samsung.jpg" /></a>
	    </span></h3>
      <p><br>
	<b>On-device Few-shot Personalization for Real-time Gaze Estimation</b><br>
	<i>Junfeng He, Khoi Pham, Nachiappan Valliappan, Pingmei Xu, Chase Roberts, Vidhya Navalpakkam, Dmitry Lagun</i>
      </p>
	</div>

    <div class="award">
      <h3>
	    Best Poster Award
	    <span class="award-sponsor">sponsored by
        <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
	    </span></h3>
      <p><br>
	<b>RT-BENE: A dataset and baselines for Real-Time Blink Estimation in Natural Environments</b><br>
	<i>KÃ©vin Cortacero, Tobias Fischer, Yiannis Demiris</i>
      </p>
	</div>

    <div class="award">
	  <span class="award-other">
	    We would like to thank
        <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
		for sponsoring our costs.
	  </span>
	</div>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>Accepted Full Papers</h2>
    <div class="paper">
        <span class="title">A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone</span>
        <span class="authors">Tianchu Guo, Yongchao Liu, Hui Zhang, Xiabing Liu, Youngjun Kwak, ByungIn Yoo, Jae-Joon Han, Changkyu Choi</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2019</button>
          <!--<button class="btn btn-poster-id">Poster #118</button>-->
  	  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Guo_A_Generalized_and_Robust_Method_Towards_Practical_Gaze_Estimation_on_ICCVW_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
        </div>
    </div>
    <div class="paper">
        <span class="title">Learning to Personalize in Appearance-Based Gaze Tracking</span>
        <span class="authors">Erik LindÃ©n, Jonas SjÃ¶strand, Alexandre Proutiere</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2019</button>
          <!--<button class="btn btn-poster-id">Poster #112</button>-->
  	  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Linden_Learning_to_Personalize_in_Appearance-Based_Gaze_Tracking_ICCVW_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1807.00664"><i class="fas fa-archive"></i> arXiv</a>
          <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
        </div>
    </div>
    <div class="paper">
        <span class="title">On-device Few-shot Personalization for Real-time Gaze Estimation</span>
        <span class="authors">Junfeng He, Khoi Pham, Nachiappan Valliappan, Pingmei Xu, Chase Roberts, Vidhya Navalpakkam, Dmitry Lagun</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2019</button>
          <!--<button class="btn btn-poster-id">Poster #114</button>-->
  	  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/He_On-Device_Few-Shot_Personalization_for_Real-Time_Gaze_Estimation_ICCVW_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        </div>
    </div>
    <div class="paper">
        <span class="title">RT-BENE: A dataset and baselines for Real-Time Blink Estimation in Natural Environments</span>
        <span class="authors">KÃ©vin Cortacero, Tobias Fischer, Yiannis Demiris</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2019</button>
          <!--<button class="btn btn-poster-id">Poster #108</button>-->
  	  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://www.researchgate.net/profile/Tobias_Fischer7/publication/336605388_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_Natural_Environments/links/5da84cd14585159bc3d5823d/RT-BENE-A-Dataset-and-Baselines-for-Real-Time-Blink-Estimation-in-Natural-Environments.pdf"><i class="fas fa-file-pdf"></i> PDF (ResearchGate)</a>
		  <a class="btn btn-default" target="_blank" href="http://www.imperial.ac.uk/personal-robotics/software/"><i class="fas fa-code"></i> Code and Data</a>
		  <a class="btn btn-default" target="_blank" href="http://tobiasfischer.info/publications/cortacero-2019-ICCVW-poster.pdf"><i class="fas fa-file-pdf"></i> Poster</a>
          <a class="btn btn-default" target="_blank" href="https://youtu.be/M3oJAP-D3Wg"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>
    <div class="paper">
        <span class="title">SalGaze: Personalizing Gaze Estimation using Visual Saliency</span>
        <span class="authors">Zhuoqing Chang, J. Matias Di Martino, Qiang Qiu, Guillermo Sapiro</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2019</button>
          <!--<button class="btn btn-poster-id">Poster #110</button>-->
  	  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/GAZE/Chang_SalGaze_Personalizing_Gaze_Estimation_using_Visual_Saliency_ICCVW_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.10603"><i class="fas fa-archive"></i> arXiv</a>
        </div>
    </div>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
  <h2>Accepted Posters</h2>
  Extended Abstracts
  <div class="paper">
  <span class="title">High-Speed Pupil Tracking based on Deep Random Forests</span>
  <span class="authors">MinJi Park, Mira Jeong, ByoungChul Ko</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE EA 2019</button>
          <!--<button class="btn btn-poster-id">Poster #116</button>-->
        </div>
  </div>

  <br>
  From the Main Conference (ICCV/CVPR)

  <div class="paper">
  <span class="title">Few-Shot Adaptive Gaze Estimation</span>
  <span class="authors">Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar Iqbal, Otmar Hilliges, Jan Kautz</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">ICCV 2019 (Oral)</button>
  <!--<button class="btn btn-poster-id">Poster #113</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1905.01941"><i class="fas fa-archive"></i> arXiv</a>
  <!--<a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>-->
  </div>
  </div>

  <div class="paper">
  <span class="title">Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks</span>
  <span class="authors">Zhe He, Adrian Spurr, Xucong Zhang, Otmar Hilliges</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">ICCV 2019</button>
  <!--<button class="btn btn-poster-id">Poster #119</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1903.12530"><i class="fas fa-archive"></i> arXiv</a>
  <a class="btn btn-default" target="_blank" href="https://github.com/HzDmS/gaze_redirection"><i class="fas fa-code"></i> Code</a>
  </div>
  </div>

  <div class="paper">
  <span class="title">Gaze360: Physically Unconstrained Gaze Estimation in the Wild</span>
  <span class="authors">Petr Kellnhofer, AdriÃ  Recasens, Simon Stent, Wojciech Matusik, Antonio Torralba</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">ICCV 2019</button>
  <!--<button class="btn btn-poster-id">Poster #111</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="http://gaze360.csail.mit.edu/">       <i class="fas fa-atom"></i> Project Page</a>
  <a class="btn btn-default" target="_blank" href="https://github.com/erkil1452/gaze360"><i class="fas fa-code"></i> Code and Data</a>
  </div>
  </div>

  <div class="paper">
  <span class="title">Mixed Effects Neural Networks (MeNets) with Applications to Gaze Estimation</span>
  <span class="authors">Yunyang Xiong, Hyunwoo J. Kim, Vikas Singh</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">CVPR 2019</button>
  <!--<button class="btn btn-poster-id">Poster #115</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="https://github.com/vsingh-group/MeNets"><i class="fas fa-code"></i> Code</a>
  </div>
  </div>

  <div class="paper">
  <span class="title">Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning</span>
  <span class="authors">Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, Song-Chun Zhu</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">ICCV 2019</button>
  <!--<button class="btn btn-poster-id">Poster #109</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1909.02144v1"><i class="fas fa-archive"></i> arXiv</a>
  <!--<a class="btn btn-default" target="_blank" href=""><i class="fas fa-archive"></i> arXiv</a>-->
  </div>
  </div>

  <div class="paper">
  <span class="title">Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis</span>
  <span class="authors">Yu Yu, Gang Liu, Jean-Marc Odobez</span>
  <div class="btn-group btn-group-xs" role="group">
  <button class="btn btn-success">CVPR 2019</button>
  <!--<button class="btn btn-poster-id">Poster #117</button>-->
  <a class="btn btn-default" target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Improving_Few-Shot_User-Specific_Gaze_Adaptation_via_Gaze_Redirection_Synthesis_CVPR_2019_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1904.10638"><i class="fas fa-archive"></i> arXiv</a>
  </div>
  </div>

  </div>
</div>
<br>
<br>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "img/people/hj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "img/people/sp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>ETH ZÃ¼rich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/zhang/">
      <img class="people-pic" src="{{ "img/people/xz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a>
      <h6>ETH ZÃ¼rich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges">Otmar Hilliges</a>
      <h6>ETH ZÃ¼rich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">AleÅ¡ Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>

<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://cai-mj.github.io/">Minjie Cai</a><h6>Hunan University</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.cc.gatech.edu/~echong8/">Eunji Chong</a><h6>Georgia Tech</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.tobiasfischer.info/">Tobias Fischer</a><h6>Imperial College London</h6></div>
    <div class="people-name"><a target="_blank" href="http://www.ti.uni-tuebingen.de/Wolfgang-Fuhl.1651.0.html">Wolfgang Fuhl</a><h6>University of TÃ¼bingen</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a><h6>ETH ZÃ¼rich</h6></div>
    <div class="people-name"><a target="_blank" href="http://www.inf.u-szeged.hu/~horanyi/">Nora Horanyi</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://hyf015.github.io/">Yifei Huang</a><h6>University of Tokyo</h6></div>
    <div class="people-name"><a target="_blank" href="http://www.ti.uni-tuebingen.de/Dr-Enkelejda-Kasneci.956.0.html">Enkelejda Kasneci</a><h6>University of TÃ¼bingen</h6></div>
    <div class="people-name"><a target="_blank" href="http://pages.cs.wisc.edu/~hwkim/">Hyunwoo J. Kim</a><h6>Korea University</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://www.cs.bham.ac.uk/~leonarda/">AleÅ¡ Leonardis</a><h6>University of Birmingham</h6></div>
    <div class="people-name"><a target="_blank" href="https://aptx4869lm.github.io/">Miao Liu</a><h6>Georgia Tech</h6></div>
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/spark/">Seonwook Park</a><h6>ETH ZÃ¼rich</h6></div>
    <div class="people-name"><a target="_blank" href="https://natanielruiz.github.io/">Nataniel Ruiz</a><h6>Boston University</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.unavarra.es/pdi?uid=2247">Arantzazu Villanueva</a><h6>Public University of Navarre</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://www.idiap.ch/~yyu/">Yu Yu</a><h6>EPFL</h6></div>
    <div class="people-name"><a target="_blank" href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a><h6>ETH ZÃ¼rich</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.ecse.rpi.edu/~cvrl/zhaor/">Rui Zhao</a><h6>Amazon Research</h6></div>
  </div>
</div>
<br>

<div class="row" id="sponsors">
  <div class="col-xs-12">
    <h2>Workshop sponsored by:</h2>
  </div>
</div>
<div class="row">
  <!--<div class="col-xs-2"></div>-->
  <div class="col-xs-4 sponsor">
    <a href="https://www.samsung.com/"><img src="img/samsung.jpg" /></a>
  </div>
  <!--<div class="col-xs-1"></div>-->
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <!--<div class="col-xs-1"></div>-->
  <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
  <!--<div class="col-xs-2"></div>-->
</div>
<div class="row">
  <div class="col-xs-12">
  	A special thank you to our industry liaisons:
	<br>
	<br>
  </div>
</div>
<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-3">
    <a href="https://www.researchgate.net/profile/Changkyu_Choi">
      <img class="people-pic" src="{{ "img/people/ckc.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.researchgate.net/profile/Changkyu_Choi">Changkyu Choi</a>
      <h6>Samsung Advanced Institute of Technology</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
  <div class="col-xs-3">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="{{ "img/people/sdm.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
  <div class="col-xs-3">
    <a href="https://scholar.google.com/citations?user=63IsxdAAAAAJ&hl=en">
      <img class="people-pic" src="{{ "img/people/kj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=63IsxdAAAAAJ&hl=en">Krister Jacobsson</a>
      <h6>Tobii</h6>
    </div>
  </div>
</div>
<br>


<hr>
<br>

<!--
<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>-->


<!--{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  -->

